{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f1624-0434-43c2-89c3-10e7df262306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import nvtabular as nvt\n",
    "from glob import glob\n",
    "import argparse\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7e0c2-41fe-45b3-8c40-1bab042d9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = '/home/jovyan/work/projects/COSME'\n",
    "config_subdir = 'configs/make_nvtab_data_config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd2828-7d8f-4044-8a65-435ece20d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = f\"{cur_dir}/{config_subdir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e379c-e4a0-415e-a697-05b24477abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"loading yaml file...\")\n",
    "config = open(config_dir, 'r').read()\n",
    "pprint(config_yaml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074367ab-64b8-4c81-8c3a-ff6067f5aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = config['in_dir']  \n",
    "out_dir = config['out_dir']  \n",
    "label_col_name = config['label_col_name']  \n",
    "input_col_name = config['input_col_name']  \n",
    "max_seq_len = config['max_seq_len']  \n",
    "row_group_size = config['row_group_size']  \n",
    "\n",
    "CUDA_VISIBLE_DEVICES = config['CUDA_VISIBLE_DEVICES']  \n",
    "do_cuda_vis_dev = config['do_cuda_vis_dev']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc79f6d-9e06-4337-a1e0-77001b62669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_files = glob(f\"{in_dir}/*.parquet\")\n",
    "split_names = [ x.split('/')[-1] for x in split_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe815d1-bb6f-4f1a-abd4-11d3956780c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"starting Dask GPU cluster...\")\n",
    "if do_cuda_vis_dev:\n",
    "    cluster = LocalCUDACluster(\n",
    "        protocol=\"ucx\",\n",
    "        enable_tcp_over_ucx=True,\n",
    "        CUDA_VISIBLE_DEVICES=CUDA_VISIBLE_DEVICES,\n",
    "        local_directory='/tmp/dask',\n",
    "    )\n",
    "else:\n",
    "    cluster = LocalCUDACluster(\n",
    "        protocol=\"ucx\",\n",
    "        enable_tcp_over_ucx=True,\n",
    "        local_directory='/tmp/dask',\n",
    "    )\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fc165-6547-4986-b629-4b4257fa1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"creating pipeline...\")\n",
    "# create the pipeline\n",
    "# nvt.ColumnGroup(\n",
    "cat_features = [input_col_name] >> nvt.ops.Categorify() >> nvt.ops.ListSlice(0, end=max_seq_len, pad=True,\n",
    "                                                                             pad_value=0.0)\n",
    "lab_features = [label_col_name] >> nvt.ops.Categorify()\n",
    "# add label column\n",
    "output = cat_features + lab_features\n",
    "# create workflow\n",
    "workflow = nvt.Workflow(output, client=client)\n",
    "\n",
    "shuffle = nvt.io.Shuffle.PER_PARTITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18b502-cb70-4a3f-9e45-6213c873ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, in_file for enumerate(split_files):\n",
    "    cur_split = split_names[i].split('.')[0]\n",
    "    \n",
    "    workflow_file = f\"{out_dir}/workflow\"\n",
    "    cur_out_file = = f\"{out_dir}/{cur_split}.parquet\"\n",
    "    \n",
    "    if cur_split == 'train':\n",
    "        print(\"fitting nvtab workflow on training data...\")\n",
    "        workflow.fit(nvt.Dataset(in_file, engine='parquet', row_group_size=row_group_size))\n",
    "        workflow.save(workflow_file)\n",
    "        \n",
    "    workflow.transform(nvt.Dataset(in_file, engine='parquet', row_group_size=row_group_size)).to_parquet(\n",
    "    output_path=cur_out_file,\n",
    "    shuffle=shuffle,\n",
    "    cats=[input_col_name],\n",
    "    labels=[label_col_name],\n",
    "    )\n",
    "    \n",
    "client.cancel(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df4d30-f2aa-43b5-b5fb-1d514919bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shutting down Dask client\")\n",
    "client.shutdown()\n",
    "print(f\"finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "mlflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
